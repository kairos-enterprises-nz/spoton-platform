#!/usr/bin/env python3
"""
Test script to verify the updated estimation DAG works with the new flag structure
"""

import sys
import os
sys.path.append('/app')
sys.path.append('/app/airflow')

import django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'utilitybyte.settings')
django.setup()

from airflow.dags.validation.hhr_estimation_dag import HHREstimationOrchestrator
from energy.validation.models import ValidationSession
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_updated_estimation_dag():
    """Test the updated estimation DAG with new flag structure"""
    
    print("Testing updated estimation DAG with new flag structure...")
    print("=" * 80)
    
    orchestrator = HHREstimationOrchestrator()
    
    # Test 1: Find latest validation session
    print("\n1. Testing validation session lookup...")
    try:
        latest_session = ValidationSession.objects.filter(
            status='completed'
        ).order_by('-completed_at').first()
        
        if latest_session:
            print(f"   ‚úÖ Found latest validation session: {latest_session.session_id}")
            print(f"      - Status: {latest_session.status}")
            print(f"      - Date range: {latest_session.start_date} to {latest_session.end_date}")
        else:
            print("   ‚ö†Ô∏è  No completed validation session found")
            return
    except Exception as e:
        print(f"   ‚ùå Error finding validation session: {e}")
        return
    
    # Test 2: Create estimation session
    print("\n2. Testing estimation session creation...")
    try:
        estimation_session_id = orchestrator.create_estimation_session()
        print(f"   ‚úÖ Created estimation session: {estimation_session_id}")
    except Exception as e:
        print(f"   ‚ùå Error creating estimation session: {e}")
        return
    
    # Test 3: Identify gaps with new flag structure
    print("\n3. Testing gap identification with new flag structure...")
    try:
        gaps = orchestrator.identify_estimation_gaps(estimation_session_id)
        print(f"   ‚úÖ Identified {len(gaps)} gaps requiring estimation")
        
        if gaps:
            # Analyze gap types
            gap_types = {}
            for gap in gaps:
                reason = gap.get('estimation_reason', 'unknown')
                gap_types[reason] = gap_types.get(reason, 0) + 1
            
            print("   üìä Gap analysis by type:")
            for reason, count in gap_types.items():
                print(f"      - {reason}: {count} gaps")
            
            # Show sample gap
            sample_gap = gaps[0]
            print(f"   üìä Sample gap:")
            print(f"      - ICP: {sample_gap.get('icp_id')}")
            print(f"      - Timestamp: {sample_gap.get('timestamp')}")
            print(f"      - Quality Flag: {sample_gap.get('quality_flag')}")
            print(f"      - Validation Flag: {sample_gap.get('validation_flag')}")
            print(f"      - SpotOn Validation Flag: {sample_gap.get('spoton_validation_flag')}")
            print(f"      - Plausibility Flag: {sample_gap.get('plausibility_flag')}")
            print(f"      - Estimation Priority: {sample_gap.get('estimation_priority')}")
            print(f"      - Estimation Reason: {sample_gap.get('estimation_reason')}")
            
            # Test 4: Flag combination logic
            print("\n4. Testing flag combination logic...")
            
            # Test priority determination
            test_cases = [
                {'validation_flag': 'F', 'spoton_validation_flag': 'S', 'expected_priority': 3},
                {'validation_flag': 'N', 'spoton_validation_flag': 'S', 'expected_priority': 2},
                {'validation_flag': 'N', 'spoton_validation_flag': 'V', 'expected_priority': 1},
                {'validation_flag': 'P', 'spoton_validation_flag': 'S', 'expected_priority': 1},
            ]
            
            for i, test_case in enumerate(test_cases):
                priority = orchestrator._determine_estimation_priority(test_case)
                expected = test_case['expected_priority']
                match = priority == expected
                print(f"      Test {i+1}: {test_case['validation_flag']}/{test_case['spoton_validation_flag']} ‚Üí Priority {priority} (expected {expected}) {'‚úÖ' if match else '‚ùå'}")
            
            # Test reason determination
            reason_cases = [
                {'validation_flag': 'F', 'expected_reason': 'failed_meter_provider_validation'},
                {'validation_flag': 'N', 'expected_reason': 'no_meter_provider_validation'},
                {'validation_flag': 'P', 'spoton_validation_flag': 'S', 'expected_reason': 'failed_spoton_validation'},
                {'validation_flag': 'P', 'plausibility_flag': 'CHECKED_NEGATIVE', 'expected_reason': 'implausible_reading'},
            ]
            
            for i, test_case in enumerate(reason_cases):
                reason = orchestrator._determine_estimation_reason(test_case)
                expected = test_case['expected_reason']
                match = reason == expected
                print(f"      Reason {i+1}: {test_case.get('validation_flag', 'P')}/{test_case.get('spoton_validation_flag', 'V')} ‚Üí {reason} (expected {expected}) {'‚úÖ' if match else '‚ùå'}")
            
            # Test 5: Execute estimation (limited test)
            print("\n5. Testing estimation execution (limited)...")
            try:
                # Only test with first 2 gaps to avoid data changes
                test_gaps = gaps[:2] if len(gaps) >= 2 else gaps
                estimated_readings = orchestrator.execute_estimation_workflow(estimation_session_id, test_gaps)
                print(f"   ‚úÖ Successfully processed {len(estimated_readings)} test estimations")
                
                if estimated_readings:
                    sample_estimation = estimated_readings[0]
                    print(f"   üìä Sample estimation:")
                    print(f"      - Original Value: {sample_estimation.get('value_calculated')}")
                    print(f"      - Estimated Value: {sample_estimation.get('estimated_value')}")
                    print(f"      - Method: {sample_estimation.get('estimation_method')}")
                    print(f"      - Confidence: {sample_estimation.get('estimation_confidence'):.1f}%")
                    print(f"      - Reason: {sample_estimation.get('estimation_reason')}")
                
                print("   ‚ö†Ô∏è  Note: Skipping actual database update to avoid data changes")
                
            except Exception as e:
                print(f"   ‚ùå Error in estimation execution: {e}")
        else:
            print("   ‚úÖ No gaps found - all data is properly validated")
    
    except Exception as e:
        print(f"   ‚ùå Error in gap identification: {e}")
        return
    
    print("\n" + "=" * 80)
    print("Updated estimation DAG test completed!")
    print("Key improvements:")
    print("- ‚úÖ Updated gap identification for new three-tier flag system")
    print("- ‚úÖ Added priority-based estimation processing")
    print("- ‚úÖ Enhanced flag-based estimation logic")
    print("- ‚úÖ Proper handling of meter provider validation status")
    print("- ‚úÖ Updated estimation result storage with correct flag updates")

if __name__ == "__main__":
    test_updated_estimation_dag() 